{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining and NLP \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text mining / Text Analytics is the process of deriving meaningful information fron Natural language text.\n",
    "\n",
    "Text Mining usualy involves structuring the input text, deriving patterns within the structured data and finally evaluating the interpreted output.\n",
    "\n",
    "Compared to the data stored in database, text is unstructured, amorphous, and difficult to deal with algorithmically.\n",
    "\n",
    "Text is the most common form of exchange of information.\n",
    "\n",
    "Text mining is the process of deriving high quality information from text.\n",
    "\n",
    "The overall goal is to turn the text into data for analysis and this done by application of NLP (Natural Language Processing).\n",
    "\n",
    "NLP is a part of Computer science and artificial intelligence which deals with natural languages.\n",
    "\n",
    "NLP refers to the artificial intelligence method of communicating with intelligent system using natural language. \n",
    "\n",
    "By utilising the NLP and components one can organize the massive chunks of textual data, perform numerous automated task and solve a wide range of problems such as automatic summarization, machine translation, name recognition, speech recognition and topic segmentation."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Structure of NLP application: Chatbot as an example:\n",
    "    Chats and comunication logs <---> NLP Layer(natural language) <--> Knowledge base(source content)\n",
    "                                      NLP Layer ---> Data Storage\n",
    "                                      \n",
    "Knowledge base is where we have the source content such as chat logs which contains a large history of chats which is used to train a particular algorithm.\n",
    "Data storage is used to store intraction history and Analytics which inturn helps the NLP layer to generate the meaningful output."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Applications of NLP:\n",
    "    Speech Recignition\n",
    "    Sentiment Analysis\n",
    "    Chat bot\n",
    "    Machine translation: (google) one language to another language\n",
    "\n",
    "Applications of NLP and Text Mining:\n",
    "    Spell checking\n",
    "    keyword search\n",
    "    information extraction from websites or documents.\n",
    "    Advertisement matching: recommndation of ads based on history.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP Components: Natural language understanding: Mapping input to useful representations.\n",
    "                                                Analyzing different aspects of the language.\n",
    "                Natural Language generation: Text planning, Sentence planning, Text realization.\n",
    "                    \n",
    "NLP Ambiguity: Natural Language understanding: Considering English language there is, \n",
    "                                               Lexical Ambiguity:Two or more meaning in single word: ex.. she is looking for a match ( can be cricket match or partner)\n",
    "                                               Syntactic Ambiguity: Two way meaning of sentence: ex.. chicken is ready to eat. (chicken or for us)\n",
    "                                               Refrential Ambiguity:ex.the boy told his father the theft.He was very upset. ( boy or father or thief)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLTK Library: Natural Language toolkit\n",
    "    used for tokenizing, stemming,tagging, classification and much more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() # to check modules installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer= CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1='Hi how are you'\n",
    "doc2='today is a very very very pleasant day and we can have some fun fun fun'\n",
    "doc3='This is an amazing experience'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_docs = [doc1, doc2, doc3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = vectorizer.fit(list_of_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = vectorizer.transform(list_of_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 18)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 7)\t3\n",
      "  (1, 8)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 16)\t3\n",
      "  (1, 17)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 14)\t1\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words) # returns tuple(doc number, index of word) and frequency of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#verify the vocabulary for repeated word\n",
    "\n",
    "print(vectorizer.vocabulary_.get('very'))\n",
    "print(vectorizer.vocabulary_.get('fun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline and Grid Search : Vectorization, Transform, Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from pprint import pprint\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  response                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spam_collection = pd.read_csv('SMSSpamCollection', sep='\\t', names=['response','message'])\n",
    "df_spam_collection.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import text processing libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#import SGD Classifier \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#import Gridsearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#define the pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('clf', SGDClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for grid search\n",
    "parameters = {'tfidf__use_idf':(True,False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing grid search now..\n",
      "parameters\n",
      "\n",
      "{'tfidf__use_idf': (True, False)}\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anzconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    5.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    5.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 6.124s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anzconda\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#perform the gridsearch with pipeline and parameters\n",
    "grid_search = GridSearchCV(pipeline,parameters, n_jobs =-1, verbose=1)\n",
    "print('performing grid search now..')\n",
    "print('parameters\\n')\n",
    "print(parameters)\n",
    "t0=time()\n",
    "grid_search.fit(df_spam_collection['message'],df_spam_collection['response'])\n",
    "print('done in %0.3fs'% (time()-t0))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
