{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"f:\\DS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      crim    zn  indus  chas    nox     rm   age     dis  rad    tax  \\\n",
      "0  0.15876   0.0  10.81   0.0  0.413  5.961  17.5  5.2873  4.0  305.0   \n",
      "1  0.10328  25.0   5.13   0.0  0.453  5.927  47.2  6.9320  8.0  284.0   \n",
      "2  0.34940   0.0   9.90   0.0  0.544  5.972  76.7  3.1025  4.0  304.0   \n",
      "3  2.73397   0.0  19.58   0.0  0.871  5.597  94.9  1.5257  5.0  403.0   \n",
      "4  0.04337  21.0   5.64   0.0  0.439  6.115  63.0  6.8147  4.0  243.0   \n",
      "\n",
      "   ptratio   black  lstat  medv  \n",
      "0     19.2  376.94   9.88  21.7  \n",
      "1     19.7  396.90   9.22  19.6  \n",
      "2     18.4  396.24   9.97  20.3  \n",
      "3     14.7  351.85  21.45  15.4  \n",
      "4     16.8  393.97   9.43  20.5  \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "bos1=pd.read_csv('boston_data.csv')\n",
    "print(bos1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bos1.iloc[:,0:13]\n",
    "y = bos1['medv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAETCAYAAABQqE86AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xm8XdP9//HX+yZIZDBLqRIlaBoRhJpCErS0RAlKtYS2Keqr2mpLdSCtqVXfXwfVhmpiaEujNCENFULNiYxEDA0l5UsREiQk8fn9sdaVnePce/bZ+4z3fp4e+5Fz9tlrr+Xm5nPWXnutz5aZ4Zxzrm0t9W6Ac841Og+UzjlXggdK55wrwQOlc86V4IHSOedK8EDpnHMleKB0zrkSPFA651wJHiidc66ErvVuQB7dtzw287Ki39wxKlfd512vzGWP/lS+H/sNt63MXPasI3NVzQVT1s5VftIJb2YuO/LkN3LVbd2y/9xb9t4sV903HrUkc9kRl3bJVffkM1flKt9//UOy/7JHaf+tLnvuT7nrqgbvUTrnXAlN3aN0zjWHFjV3qGnu1jvnmoLU3BevNW+9pBGSzqp1vc65+pFaUm2NqqY9SkldzWwiMLGW9Trn6ktqyHs0qVU8UEo6HjgTMGAusAp4DdgZmClpHjDYzE6TNA5YBuwAbAWcCJwA7Ak8ZGajKt0+51w9NG5vMY2KBkpJHwfOAfY2s1ckbQhcCmwHHGBmqySNKii2ATAcGAFMAvYGvgxMlzTIzGZXso3Oudpr5MvqNCrd+uHABDN7BcDMXov7/2JmbU3mmmQhzfo84CUzm2dm7wGPAX0LD5Y0WtIMSTNWvvl0hZvvnKuGFnVNtTWqSgdKES65C73VTpl34p/vJV63vv/AT87MxprZYDMb3LXntpkb6pyrnUrezJF0kKQnJD1d7MawpK0kTZU0V9I0SVvkbX+lA+VU4GhJGwHES2/nXCdXqUApqQtwGXAw0B84VlL/gsMuAa42s4HAGODCvO2vaF/XzB6TdD5wt6RVwKxKnt8515wqOEa5O/C0mS0M59WfgcOA+Ylj+gPfiK/vAm7OW2nFBwXMbDwwvp3PxwHj4utRif3PAgMS70fhnOsQRMWmB30YeD7xfhHwiYJj5gAjgV8AhwO9JG1kZq9mrbS5b0U555pCS0vXVFvyZm3cRhecqljELbwvciawn6RZwH7Af4DsmWTwJYzOuRpIe+ltZmOBse0csgj4SOL9FsALBed4ATgi1KuewEgzy5V6qqkDZZ5UaaceMC5X3YuePDZz2fNm9cpV9wPfeqf0QW0YcvV6ueqefWq+VGe7j8x89cP432+Sq+53VmW//NuoW/Y0aQCj71o/c9kpZ76eq+5PXtUjV/nnvpmreFSxi9fpQD9JWxN6iscAn08eIGlj4LU4zfBs4Kq8lfqlt3Ou6ip119vMVgKnAbcBjwM3xJvIYySNiIcNBZ6Q9CTQBzg/b/ubukfpnGsOlVyZY2aTgckF+36YeD0BmFCxCvFA6ZyrATX5xasHSudc1bW05HucRb15oHTOVV2zJ8XwQOmcqzq/9M5A0snAyfHtesCzwG6EmfSHEHJUHmZmL9Wjfc65ymr2HmVdWm9mvzWzQYTguIiQs7IH8KCZ7QTcA3ylHm1zzlVesz8Kot4t+wVwp5lNAt4Fbon7H6FILkpYMx/ltOsnFzvEOddgREuqrVHVbYwyZjrfijB5FGBFTOAL4fERRduWXOL0hydvS/VQdedcfamluW+H1GuMclfCwvUhcZmRc64D84eLZXMasCFwV/wBzqhTO5xzNdDIl9Vp1CVQmtmJRXZ/OfF5xZcgOefqp5Fv1KTR3AMHzrnm4JfezjlXQhcPlHVz3vXZf/h58kkCbLHdnzKX/fQfTs1V937Dny99UBtuuCXfJdDuxyzOVf7xiYMyl/3YUY/mqluvvp257Dsj+uWq+5ZR2XNKHvKVfPkoH766Ae6Xeo/SOedKaO4hSg+UzrnqM+9ROudcCc0dJz1QOudqoKW5I2XmkQNJ95d5/FBJt5Q+0jnX4bQo3dagMvcozWyvSjbEOdeBNXAQTCNPj/LN+OdQSdMkTZC0QNJ1iusSJR0U991LfM5u3H+upDMT7x+V1FdSD0m3SpoT930ux/+bc65RSOm2BlWpm/Y7A2cA/YGPAntL6gZcARwKDAE+lOI8BwEvmNlOZjYAmFJ4QDLN2tIZnmbNuaaglFuDqlSgfNjMFsVMQLMJuSR3AJ4xs6di+rRrU5xnHnCApIslDTGzNwoPMLOxZjbYzAb3GvzpCjXfOVdVTT5GWalA+U7idTKXZFv5IlcW1N0NwMyeBHYlBMwLJf2wSFnnXLNp8h5lNacHLQC2lrSNmf0LSK4ZfJbwbBwk7QJsHV9vDrxmZtfGMdBRVWyfc65GrEtzL82pWqA0s+WSRgO3SnoFuBcYED++EThe0mxgOvBk3L8j8DNJ7wErgFOq1T7nXA01cG8xjTzTg3rGP6cB0xL7T0u8nkIYqywsuwz4ZJHTPgvclrVNzrkG1cB3tNPwlTnOuepr4Bs1aWj187yaz3cevjNz45evyvcX95+3s3/HTD7xN7nq3veKr+Uq37WOv7O7b7I8c9mXlnXJVffaLdl/1zfttipX3XMXr5O57Ie6r8xV98z/Zq8b4J5D9879G7Pt4Ven+uE/fdPxJeuSdBDhCa5dgCvN7KIixxwNnEu4oTzHzD5fVoMLeI+yk6lnkHSdWEtlbuZI6gJcBhwILAKmS5poZvMTx/QDzgb2NrPFkjbNW29z34pyzjWHlpRbabsDT5vZQjN7F/gzcFjBMV8BLjOzxQBm9nIlmu+cc9VVuSWMHwaSKf4XxX1J2wHbSbpP0oPxUj0Xv/R2zlVfyiGfOKVwdGLXWDMbW+JMheOfXYF+wFBgC+CfkgaYWeZnalQ1UEoaB9wSHz/rnOukLOVd7xgUx7ZzyCLgI4n3WwAvFDnmQTNbATwj6QlC4JyeusEF/NLbOVd9lbv0ng70k7S1pLWBY4CJBcfcDAwL1WpjwqX4wjzNr2iglHS8pLkxTdo1cfe+ku6XtFDSkfG4npKmSpopaZ6kw+J+T7PmXEfURem2EsxsJXAaYWHK48ANZvaYpDGSRsTDbgNelTQfuAv4tpm9mqf5Fbv0lvRx4BzCLflXJG0IXApsBuxDWKEzEZgALAcON7MlMeI/KGkiq9OsfSaec71Ktc85V0cVXJljZpOByQX7fph4bcA341YRlexRDgcmmNkrAGb2Wtx/s5m9F+c59Yn7BFwgaS5wB+GuVR9SpFlL5qOcc5M/WcK5ptDk2YMqGShF8bRq7xQcA3AcsAmwq5kNAl4CuqVJs5bMR7nT4YdUsPnOuarxfJTvmwocLWkjgHjp3Zb1gJfNbIWkYcBWsczmwNtmdi1wCbBLBdvnnKuXJg+UFRujjAOq5wN3S1oFzGrn8OuASZJmEDKiL4j7Pc2acx2QpbhR08gqOo/SzMYD49v5vDU12yvAnkUOeRZPs+Zcx+Np1pxzroQGvqxOwwOlc676mnxpS1MHyhtuy56n74FvvVP6oHbsN/z50ge1IW8+yXu+clnmshPuOz5X3Wcc91yu8rfeMyxz2W2PejhX3SzL/vvS50vb5qp67NDMy4wZceqSXHXf/rteucpXhF96O+dcCX7p7Zxz7fO73s45V4r3KJ1zrgQfo3TOuRK8R+mccyU0d5ys7uwmSX0lPS7pCkmPSbpdUndJg+KzLOZKuknSBpK6SpouaWgse2FcEumca3LWolRbo6rFNNB+hCeifRx4HRgJXA1818wGEjIF/Sgm5BwFXC7pQEJuyvMKT5ZMs7Z0xuTCj51zjahLS7qtQdWiZc+Y2ez4+hFgG2B9M7s77hsP7AshsQZwDTAJOCk+jnINyTRrvQZ/uvqtd87lV7nH1dZFLcYok0tgVgHrlzh+R0LPs0+J45xzzaLJ73rXI4a/ASyWNCS+/yJwN4CkI4CNCD3MX0oqFVSdc83A81FmcgLwW0nrEp6OdmJ8ds5FwP5m9rykXwO/iMc655pZAwfBNKoaKM3sWWBA4v0liY/3KFJku8Sxv6xey5xzteRLGJ1zrpQmH6Ns6kB51pHZyw65Ot+TcG+4Jfvw7g9m5qo6d6q0I/e+OnPZsXeOylX3Hje+nLnsmWM2zVV3n+6rMpfdfN0PPBC0LF+7f4PMZWeOXytX3YMPWZSr/FO35yoe+KW3ayZ5gqRzmXmgdM65Epo7TnqgdM5VXyMvT0zDA6VzrvoaeHliGh4onXPV19wdyvqtrlTQ3F8zzrlUWlrSbWlIOkjSE5KelnRWkc9PljRP0mxJ90rqn7v9eU9QjkTatd8AM4FVki6W9IikOyTtLmmapIWSRtSybc656pHSbaXPoy7AZcDBQH/g2CKB8I9mtqOZDQJ+Clyat/316NFtD1xtZjvH99PMbFdgKfAT4EDgcGBMHdrmnKuCSgVKYHfgaTNbGLOL/Rk4LHmAmSWf79sDsLztr0eg/LeZPRhfvwtMia/nAXeb2Yr4um+xwsl8lP+8wfNROtcMJKXaUvgw8Hzi/aK4r7C+r0n6F6FHeXre9tcjUL6VeL3CzFqj/XvElGxm9h5t3GhK5qMccrTno3SuGaQdo0x2hOI2uuBUxaLpB3qMZnaZmW0DfBf4ft72+11v51zVpb1ta2ZjgbHtHLII+Eji/RbAC+0c/2fg8nS1t83vOjvnqq6CY5TTgX6Stpa0NnAMMHHNutQv8fYzwFN521/THmWRtGs9E6/PLTi2J865DqFSC3PMbKWk04DbgC7AVWb2mKQxwAwzmwicJukAYAWwmArktPVLb+dc1VUyy5qZTQYmF+z7YeL11ytXW+CB0jlXdS2+1rt+Lpiyduays0/Nl19w92MWZy67/Sn5cmGecdxzmcvmzSc5evi4XOXf+nf2G5DbfDv7zxxAy1dmLttrj41z1f2zYdl/33b9Qr7/779dX//n9DX7GrymDpTOuebQ5AnOPVA656rPA6VzzpXggdI550po8ns5Hiidc9WXNoVao/JA6ZyrOjV5l7KqcV7SzTHX5GOti9slfUnSkzHv5BWSfh33byLpRknT47Z3NdvmnKudCi5hrItqd4hPirkmBwOnS/ow8ANgD0LeyR0Sx/4C+F8z2w0YCVxZ7ITJ7CJvPjCpuq13zlVEswfKal96ny7p8Pj6I8AXCTknXwOQ9Bdgu/j5AUD/RE663pJ6mdnS5AmT2UW2/N9puRNyOueqr5GDYBpVC5SShhKC355m9rakacATwMfaKNISj11WrTY55+qjyYcoq3rpvR6wOAbJHQiX2+sC+0naQFJXwiV2q9uB01rfSBpUxbY552qopUu6rVFVM1BOAbpKmgv8GHgQ+A9wAfAQcAcwH2hdBHs6MFjSXEnzgZOr2DbnXA35GGUbzOwdwpPS1iBphpmNjT3Kmwg9SczsFeBz1WqPc65+Uj4Pp2HVYxrouZJmA48CzwA316ENzrka8h5lmczszEqda9IJb2Yuu/vIV3PV/fjEfEOoF8x5JXPZW+8ZlqvuPW58OXPZPGnSAHps9ZPMZZ954thcdXdR9kGwKYvezlX3KROzJ+y/b3z29HAAw//UO1f5+SflKg40dhBMw1fm1EGeIJlXniDpXFZdfAmjc861r9mnB3mgdM5VXYuae22IB0rnXNV5jzIjSecCbwK9gXvM7I56tcU5V11NPkRZ/x5l8jGTzrmOqdkvvWsa6CWdI+kJSXcA28d94yQdGV9fJGl+XJ1zSS3b5pyrnq5KtzWqmvUoJe0KHAPsHOudCTyS+HxD4HBgBzMzSevXqm3Ouepq9jHKWvYohwA3mdnbZrYEmFjw+RJgOXClpCOAojN8k/koJ4yfUt0WO+cqQrJUW6Oq9Rhlmz8JM1spaXdgf0LP8zRgeJHj3s9HOee1Wxr3J+uce5/3KNO7BzhcUndJvYBDkx9K6gmsZ2aTgTMAT7PmXAfRknJrVDXrUZrZTEnXA7OBfwP/LDikF/A3Sd0AAd+oVducc9XVtaVyF3+SDiI8OqYLcKWZXVTw+TrA1cCuwKvA58zs2Tx11vTS28zOB85v55Dda9UW51ztVKq3KKkLcBnhmVuLgOmSJprZ/MRhXyIkDd9W0jHAxeRM4djIvV3nXAfRonRbCrsDT5vZQjN7F/gzcFjBMYcB4+PrCcD+ypkQ0wOlc67qWmSpthQ+DDyfeL8o7it6jJmtJDxFYaM87a/7ypw8Rp78RumD2jD+95vkqvtjRz2auewnv1X491qebY96OHPZM8dsmqvubb69OFf5PDklt97+T7nq7tG9T+ayvQ7KlwP07vPfzVx2r2+vylX3lPOX5CpfCWnveksaDYxO7BobZ7q8f0iRYoURNs0xZWnqQOmcaw5pL12T0//asIjw6OtWWwAvtHHMovjImfWA11I2oSi/9HbOVV3XFku1pTAd6Cdpa0lrE+ZcFy5emQicEF8fCdxpZt6jdM41tkpNOI8LU04DbiNMD7rKzB6TNAaYYWYTgd8D10h6mtCTPCZvvR4onXNVV8lL17goZXLBvh8mXi8HjqpglY176S1pqKRb6t0O51x+FbzrXRc171HG+Uwys/dqXbdzrj6afa13TQKlpL7A34G7gD2B2ZJ2BLoDE8zsR/G4g4D/B7xCSMPmnOsAGvbSNaVatn974Goz2xn4lpkNBgYC+0kaGNd4X0FIljEE+FCxkyTTrC1ZeGet2u6cy6FLi6XaGlUtA+W/zezB+PpoSTOBWcDHgf7ADsAzZvZUvJV/bbGTmNlYMxtsZoN7f/QDWdiccw2ogksY66KWY5RvAUjaGjgT2M3MFksaB3SLxzTuV4pzLjO/9C5fb0LQfENSH+DguH8BsLWkbeL77GvdnHMNxe96l8nM5kiaBTwGLATui/uXx3Wet0p6BbgXGFDr9jnnKq+RL6vTqEmgjEkzByTej2rjuCmEsUrnXAeylgdK55xrXyNfVqfhgdI5V3V+6V1H1i17899Zle9vTq8WfZpuKmvnnS+2bGXmon2658ttqOXZ6wbooi6Zy+bJJwnw1rKXMpfttVa++555/saV4+8boNda9f9n7oHSOedK6OKB0jnn2uc9SuecK6GSj6uth5pMOJe0vqRTa1GXc67xdEm5NaparcxZH/BA6Vwn1exrvWsVKC8CtpE0W9L/SpoqaaakeZIOA5C0m6S5krpJ6iHpMUm+Mse5DsCXMKZzFjDAzAbFp6Kta2ZLJG0MPChpoplNlzQR+AkhT+W1Zpb9mbDOuYbR7He965EUQ8AFkuYCdxAeVt46QW4McCAwGPhp0cLJfJRP3lGL9jrncvJL7/IdB2wC7Gpmg4CXWJ1mbUOgJ9ArsW8Na+Sj3O6AWrTXOZdT15Z0W6OqVdOWEoIfhIeRv2xmKyQNA7ZKHDcW+AFwHXBxjdrmnKuyLrJUW6OqVfagVyXdJ+lRwgPMd5A0A5hNyEOJpOOBlWb2R0ldgPslDTczf96Dc02ugTuLqdRswrmZfb7EIc8CV8djVwGfqHabnHO10cjjj2n4yhznXNV5oHTOuRLWavIljE0dKFv23ixz2Y26LclV9zsj+mUuu2m3fGmz+nxp28xlN1/3jVx199pj41zlpyzKnp6u10HDctWdJ1Xa/93wx1x13zl6VOayKwb0Kn1QO6R8v+uV4D1K55wrodkDZbPfjHLONYEuSrflIWlDSf+Q9FT8c4Mix2wl6ZG4nPoxSSenObcHSudc1dVorfdZwFQz6wdMje8LvQjsFRe7fAI4S9LmJduft2XOOVdKS8otp8OA8fH1eOCzhQeY2btm9k58u07aajO3TdIZktbNUG5UMoJLulJS/6ztcM41vrVa0m059TGzFwHin5sWO0jSR2KuieeBi83shVInznMz5wzgWuADtzEldYmTxosZBTwKvABgZl/O0QbnXBNIe1ktaTQwOrFrrJmNTXx+B/ChIkXPSdsWM3seGBg7bDdLmmBm7T55rmSglNQXmAI8BOwMPAncA2wO3CXpFTMbJulN4FLgU8C3JA0HDiWkTLsf+CowkpAZ6DpJy4A9gb8DZ5rZDEnHAt8jZBi61cy+m/Z/3jnXuNLe9Y5BcWw7n7eZCUfSS5I2M7MXJW0GvFyirhckPQYMASa0d2zazu72hMg+EFgCrE3oEQ4zs9bJbT2AR83sE2Z2L/BrM9vNzAYQguUhZjYBmAEcZ2aDzGxZ4n9yc0IijOHAIGA3SR8YY0imWXvjnxNTNt85V081SrM2ETghvj4B+FvhAZK2kNQ9vt4A2Bt4omT7UzbgeTO7L76+FtinyDGrgBsT74dJekjSPELw+3iJOnYDppnZf81sJSGD0L6FByXTrK03ZETK5jvn6qlGN3MuAg6U9BQhr+1FAJIGS7oyHvMx4CFJc4C7gUvMbF6pE6cdoywcYCg24LC8dVxSUjfgN8BgM3te0rm0kV8yocmnpDrn2qIa/Os2s1eB/YvsnwF8Ob7+BzCw3HOnDeJbStozvj4WuJc1c0wWag2Kr0jqCRyZ+Kytcg8B+0naOKZZO5YQ8Z1zTa7ZM5yn7VE+Dpwg6XfAU8DlwLvA3yW9mBinBMDMXpd0BTCPkD5teuLjccBvEzdzWsu8KOls4C5C73KymX1gjME513yafcJ22kD5npkVLvX5VdwAMLOeyQ/N7PvA9wtPZGY3suZY5tDEZ38E8mUfcM41HDVw9vI0PCmGc67qGviqOpWSgdLMngX8+drOucxqcTOnmpq6R3njUdnz7I2+a/1cdd8y6vXMZS+Y0ztX3WOHZq/7a/d/IKFKWX42LF8+y1Mm9ix9UBvuPv/dXHXnufjLk08S4NQDxmUuO33Ocbnq/sRl2X/mAP8+M1dxoPmf693UgdI51xyaPE56oHTOVZ9fejvnXAlNHifzTW+S1Dc+q7tw/zRJgzOcb5SkX+dpk3Ou8Sjl1qi8R+mcq7pGXnWTRiUmzHeVNF7SXEkTCpP5Sro8Zvt5TNJ5if27Sbpf0hxJD0vqVVDuM5IekJTvsX/Oubqr0aMgqqYSPcrtgS+Z2X2SrgJOLfj8HDN7La7fnippILAAuB74nJlNl9QbSKZcOxz4JvBpM1tcgTY65+qoyTuUFelRlkrBdrSkmcAsQqq1/oTg+qKZTQcwsyUxtRrAMOC7wGeKBclkPsoJ46dUoPnOuWqT0m2NqhI9yjZTsEnaGjgT2M3MFksaR8gspCLlWi0EPgpsR0jyu+bJExmQ5752S+P21Z1z72v2pBiVaH+xFGytegNvAW9I6gMcHPcvADaXtBuApF6SWoP2v4EjgKsllUr265xrAs3eo6xEoGxNwTYX2JCQgg0AM5tDuOR+DLgKuC/ufxf4HPCrmGn4HyQS+5rZE8BxwF8kbVOBNjrn6qiz5KMsKibMKPao2aGJY0a1UXY6sEfB7nFxw8xmtXFu51yTaeAYmIrPo3TOVV0j9xbT8EDpnKu6Jo+TzR0oR1zaJXPZKWdmT1UGcMhXspc/5Fvrlj6oHSNOzZ5ebub4tXLVvesX8k1rvW/8ytIHtWGvb6/KVbeWZa97xYC2Hg+VTp5UabvtdF2uut9+7oe5yleCZzh3zrkSvEfpnHMl+Bilc86V0OwTzj1QOueqrpEnk6dR0UAv6c0Sn38v5XlSHeecaxbNnZGy1j3itAHQA6VzHYhS/teoqhIoJW0m6R5JsyU9KmmIpIuA7nHfdfG4myU9EnNVjo77PnCcc665SS2ptkZVrZZ9HrjNzAYBOwGzzewsYJmZDTKz1kllJ5nZrsBg4HRJG7Vx3PuSadaWzvx7lZrvnKsk0ZJqy1WHtKGkf0h6Kv5Z9NnMkraUdLukxyXNl9S31LmrFSinAydKOhfY0cyWtnHc6TEpxoPAR4B+pU5sZmPNbLCZDe61y8GlDnfONYSajFGeBUw1s37A1Pi+mKuBn5nZx4DdgZdLnbgqgdLM7gH2Bf4DXCPp+MJjJA0FDgD2NLOdCFmGuhUe55xrfjW69D4MGB9fjwc++8F2qD/Q1cz+AWBmb5rZ26VOXK0xyq2Al83sCuD3wC7xoxWSWtfQrQcsNrO3Je3AmpmEksc555peTXqUfczsRYD456ZFjtkOeF3SXyXNkvSz+JiadlVrHuVQ4NuSVgBvAq09yrHA3PhoiJOAk2MeyycIl98UHldsnNI511zS3tGON3VHJ3aNjU81aP38DuBDRYqek7IpXYEhwM7Ac4Rnd40idOjaLVQxZtYz/jme1V3g5OffJTwPp1XRQcYixznnmphIl8Am+aiXNj4/oM06pJckbWZmL0rajOJjj4uAWWa2MJa5mXA1226gbNz78c65DkNSqi2nicAJ8fUJwN+KHDMd2EDSJvH9cGB+qRN7oHTO1UBNxigvAg6U9BRwYHyPpMGSrgQws1WEBx5OlTQvVnpFydabNW+euPmvZ38K40FX9chV98OntDXjqbSj79goV91/2O+1zGUPOrLkTIh23Xx9n1zlj7qpd/a6R2bPwwnQa63sv+t58yl+4rKemcs+e+Zmueped8sxucove+5PuSPY2yv/meoHuG7XIQ25PMeTYjjnaqC5L149UDrnqq6R13Gn4YHSOVd1jbyOO426t17SNEmD690O51z11GKtdzV5j9I5VwPNfemdKYRL6itpgaQrYxq16yQdIOm+mLljd0k9JF0laXpcKnRYLNtd0p8lzZV0PdA97j9F0k8TdYyS9KuK/F865+qqRvMoqyZPj3Jb4CjCcqPphNRq+wAjCIl35wN3mtlJktYHHo7Lj74KvG1mAyUNBGbG800AHgC+E99/Djg/R/uccw2jcYNgGnkGBZ4xs3lm9h7wGCG9kQHzgL7AJ4GzJM0GphEyA21JyCp0LYCZzQXmxtf/BRZK2kPSRsD2wH2FlSbzUd4wbkqO5jvnaqUzj1G+k3j9XuL9e/G8q4CRZvZEslDsXrc1+fR64GhgAXCTFZkNn1wLmmfCuXOudho5CKZRzdbfBvyPYmSUtHPcfw9wXNw3ABiYKPNXQg65YwlB0znXATT7GGU1A+WPgbUI6dIeje8BLgd6xvRq3wEebi1gZosJY5tbmdnDOOc6iJaUW2PKdOltZs8CAxLvR7Xx2VdkNj8lAAANqUlEQVSLlF0GHNPOuQ/J0ibnXOPylTnOOVeSB0rnnGtXiqctNDQPlM65qmv2S2/MrMNuwOh6lfe6ve7OUHdn2Rr3NlNljC59SNXKe91ed2eou1Po6IHSOedy80DpnHMldPRA2eZjL2tQ3uv2ujtD3Z1CUz9czDnnaqGj9yidcy43D5TOOVeCB0rnnCuhQwZKSQMljZB0ROtWo3qPktQrvv6+pL9K2qXaZWOZH0vqmnjfW9Ifyv1/yENSj1rWl5ek/kX2DS3zHCMkXRK3Q8ssu2GRfVunLHtNmn3tlP+FpL3SHt/ZdbhAKekq4CpgJHBo3FJlJJK0iaTvSRobn/dzVTxfWj8ws6WS9gE+BYwnpJWrdlkIy1Efil8SnyQ8nuORtIUlHRKfbfSapCWSlkpakrLsXpLmA4/H9ztJ+k3Ksj+NQX0tSVMlvSLpCynKTZI0sa0tTd3ADZK+q6B7fEbThSnLIulC4OuE1IDzgdPjvrQmSeqdOF9/YFLKsh8vaEsXYNcy6p4JfF/S05J+Jn8SavvqvTSo0hswP0fZ+4GLCVnWR7ZuZZSfFf+8EPh8cl81yybOcQCwDHgB2LbMsk8Tkigrw8/tIeAjyfYCj6YsOzv+eTjhy2FDYE6KcvvF7ReEJM+tX4p/BC5IWXcP4NeEZzU9CpwNtJTx/z03eTzQBZhbRvnPAHcDPQlB7jFgUIkyZwNLgZXAkrgtBV4FLszwd7ch8BVgKvBUueU7y9YRk2I8IKm/mc3PUHZdM/tujrr/I+l3hIB1saR1SN9rz1MWSfsSgsYYYEfg15JOMrMXUp7ieUJwyzRfzMyeL8hQvSpl0bXin58G/mRmr6XJdG1md0MYcjCzfRMfTZJ0T8q6VxC+WLoTnun0jIVnQJVjfeC1+Hq9cgqa2a2S1gJuB3oBnzWzp0qUuRC4UNKFZnZ2mW0tZltgB8JzrrL8m+kUOmKgHE8Ilv9HeI6PADOzge0XA+AWSZ82s8kZ6z4aOAi4xMxel7QZ8O0alAW4BDiq9QsijsveSfhHkMZ3gMmS7ibxPCQzuzRF2efjeJdJWhs4nXgZnsIkSQsIAetUSZsAy1OWBdhE0kfNbCG8P8a3Scqy04G/AbsBGwG/k3SkmR2ZsvyFwCxJdxF+z/Yl9PjaFS/xk19IvYGFhEenYGanlzqHmZ0taQOgHyHIt+5P9SUh6WLgCOBfwA3Aj83s9TRlO6MON+Fc0tPANwlPg3y/d2Bm/05RdimwLqGn8S6rg2zvdguuLr9lsf1m9lya8vEcm7LmL36qspK6mNmqgn0bmdmrKcvfDrzJB39u56UouzGhN3sA4Wd2O/D1MureAFhiZqskrQv0NrP/S1n2IMLKkoVxV1/gq2Z2W4qyg81sRsG+L5pZOTdFNiMEWgEPpWm3pBPa+9zMxqc4x5cJ46NbALOBPYAHzGx4ynafDEwws1fSHN/ZdcRAeWfaX5ZiZYGfm9mtiX1XmNlXUpafR+gpiBDstgaeMLOPt1swlB0B/BzYHHiZ8GjfBWnKxvLrAecSejVGGPsaY2ZvpCw/w8xqNqAvabiZ3ak2ZiSY2V/LONc6rO45LzCzd9o7vkj5sr6cJO1gZgvampVgZjOL7S9ynh7A8tYvuHhDZh0zeztF2XmEAP2gmQ2StANwnpl9rkS5dmdSpG17Z9MRL70XSPoj4e5h8hIyzT+8vsB3JO1qZmPivtR3Es1sx+T7+Ev5gecGteHHhF7BHWa2s6RhhKdRpnUV4YbEUfH9F4E/EC6v0rhD0ifN7PYy6gRA0i+L7H4DmGFmf2uj2L6EoYFDWf3lkvwzdaAkXH5uTwh2O8XL16tTtPtQ4FJWfzltRRgyKPXl9E1CarKfF/nMgLRf1FMJvfA34/vuhN54mmk7y81sucLTC9eJgXv7FOVa29wNGAzMIfzMBxJuyu2Tsu2dSkcMlN0JAfKTiX1p/+G9DuwP/FLSJKDkNJX2mNlMSbulPHyFmb0qqUVSi5ndFceR0trGzEYm3p8naXaaggp3T75D+JJ4hzD0UM6wQzdCj+4v8f1Iwh3cL0kaZmZnFCmzVNI3CcG9NUBC2898b6vtPwKGAv2BycDBwL1AyUAJ/IQMX05m1pq/8WAzW2M8VVK3IkXa0s3MWoMkZvZmHHpIY5Gk9YGbgX9IWkyY7dAuMxsW2/lnQsLeefH9AODMMtreqXS4QGlmJ+YoLjNbSbipMIrwD26D1IXDP/xWLcAuwH9TFn9dUk/Cc8+vk/QyYQpIWssk7WNm98a27E24QVKSmZmk2WaWeoJ7gW2B4fFnh6TLCT2jAwljnsX0jH9uT7iE/BshWB5K+BmkdSSwE2Fq0omS+gBXpiyb98vpfsLfcal9bXlL0i6tl7uSdiX939nh8eW58WbSesDfU9YLsENrkIzne1TSoDLKdyodJlBK+o6Z/bTIHUWAVHcSgd8mjh8Xx4G+VkYzeiVerwRuBW5MWfYwwt3ebwDHEX7xx7RbYk2nAOPjWCXAYqDdmwYFHpC0m5lNL6NMqw8T5iS2jof2ADaPN2eKjhe23iSKN5F2MbOl8f25rO6ZprHczN6TtFJh8vbLwEdTli325bSiVCFJHyL8P3eXtDOre8O9CTcD0zoD+Iuk1p7gZkC7Y4yJNlxjZl+ENaZKXUMYcknjcUlXAtcS/r18gfQzFTqdDhMoWf2XPKPdo9phZr8reP8IcFIZ5UveIW6n7FuJtyXvehbxOPBTYBvC3L43gM8SJkWnMQw4WdKzwFuUN63qp8BsSdNYPU3mgniz4o4SZbckzDBo9S5hrLikOGQwN16CXkFYifQm8HCa8oTxubdZ88upZ7slgk8Bowh3nJPTp5YC30tZN2Y2Pd6E2Z7wc1tgZiUDdZR3Zc6JhC/Xr8f391DeSrBOpUPd9Y6/LBeZWTnzDytR7yTaGVszsxEpznEEYVXQpoR/NOVOTZpCGGOdSWKyt5kVu+FQrPxWxfanmVYVy29O6M0sIPQoF6WZ0yfpHMIc0psIP8PDgevjxOo09T5iZrvG130JU4tSfTlImlk43CBpbsovBySNNLO0VwxtnWMAYXw1ede9zfFVSWcTgnF3QpCH8LvyLjC2nEnokroDW5rZExma3ql0qEAJ+aYH5ahzv/jyCOBDhMsZCDcGnjWzkr2MOP/zUDPLdPkj6VEzG5ClbF4VmNO3CzAkvr3HzGaVUfdlwLhyhgwknQKcSuh9P534qBdwn5mlvokn6TOE3l0y0KUaMmnrRlSaCe/KuTInTkf7GbC2mW0dxyfHpPlS75SsAdZRVnIjTH+YSOjdHNG61ajue9Lsa6PsfTnrHgvsWKef+TxCoGhdt70DoVdYi7rnE8aD/0UYZphHifXWhEvsvsCfCFOCWrcNy6z7t4S7688DP4p1/77Mn1sLcW070AeYlLLs3kCP+PoLhCGArcqo+5H4c0iuz0+9Tr2zbR1pjLLVhoQEAcneTLnz8rIqezldYsL1DEnXE6Z7lDv/E8L8t1GSnqH8pZt5ZZ3TVwkHl1vAwiT8Nyhvnmoxe5nZwHi5fp6kn1Pe79kyy34j6nLCnNGdCFO7fk8I2vu1W2q1lWb2hlKsq3cd62ZOqxbC8rnX4f3lcanG6SrgG8A0SWsspytRpjWHoRHGnLLM/4QMAaOCMs3pqwRLOYZaJa1zKN+OY7SvElZjpTUjx42olWZmkg4DfmFmv1eJpZEFHpX0eaCLpH6E9fn3l1G+U+mIY5SzzGznUvuqWH+m5XSSxlMkwJtZ6rvujSCO164HTDGzd0sd38wk/QD4FWGRwmWEL7YrzOyHGc7Vl/JuRN0NTCHcvd6XMF93thWsDmun/LrAOaz+Yr6NkBijrOWfnUVHDJRzgKFmtji+3xC4O+0vUAXq34vQk3y/t27pltPVNcC78khqAfYws/vj+3UIK21Krq1XBdZbx7mcnwemm9k/FRKyDE3zuxbLDyYEyr6s/l2t1VBN0+mIgfJ4QqqrCYRv+KOB862MjDA56r6GcCd1Nqun6JilmOxe7wDvyifpATPbM0O5uxJvk/8AW8eVqz5rQ9IThCWLj1Jmlq3OqMMFSqA1pf5wwi/eVMuWxDdLvY8D/S3DD7WeAd5lI+k8wp32v2b8O+9OmKa0D+Hv/J/A5VawfrygzFKKz9ktd97tvWbmCTBS6pCBsl4k/QU43cxezFi+LgHeZRODVg/C9KTllB+sbiA8yuG6uOtYYH0zO7oKzS2se/9Y31SyzbLoVDriXe962hiYL+lh1vzlSzWJNwZGD45Nwsx6lT6qXdub2U6J93fFIZhaOJFw03EtVl9612oaXdPxQFlZ59a7Aa52JE01s/1L7WvHLEl7mNmDsewngPsq3c427OTj3+l5oKwgi1lcXMemkHNyXWDjOI0rmT1o8xTlWzPhrwUcL+m5+H4randF8aCyP4Sv0/FAWQGtA+NFBtrLGrNyTeOrhBRpm7Pms9OXEuZTlpLqOfNVtg9wQp1WcjUdv5njXEaS/gdYmzLuWjeKvNmiOhsPlM5lFGc5vEEd7lq72vJA6VxGkuYU3LUuus81v5Z6N8C5JjZL0h6tb2p819rVkPconcsorsTaHmh9DviWhEdyvIffGOlQPFA6l1FbN0Ra+Y2RjsMDpXPOleBjlM45V4IHSuecK8EDpXPOleCB0jnnSvBA6ZxzJfx/STBOpVFbWGwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names=[]\n",
    "correlations = bos1.corr()\n",
    "sns.heatmap(correlations, square = True, cmap = 'YlGnBu')\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=90)\n",
    "type(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.33, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6921228999038543"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm= LinearRegression()\n",
    "lm.fit(x_train, y_train)\n",
    "#lm.predict([])\n",
    "lm.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is used to estimate real values (cost of houses, number of calls, total sales etc.) based on continuous variable(s). Here, we establish relationship between independent and dependent variables by fitting a best line. This best fit line is known as regression line and represented by a linear equation Y= a *X + b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Item_Weight  Item_Visibility  Item_MRP  Outlet_Establishment_Year  \\\n",
      "0     6.800000         0.037490   48.6034                       2004   \n",
      "1    15.600000         0.172597  114.8518                       1997   \n",
      "2    12.911575         0.054721  107.8254                       1985   \n",
      "3    11.800000         0.098312   81.4618                       1998   \n",
      "4    17.850000         0.046600  125.1388                       2004   \n",
      "\n",
      "   Item_Outlet_Sales  Item_Fat_Content_LF  Item_Fat_Content_Low Fat  \\\n",
      "0           291.6204                    0                         1   \n",
      "1          2163.1842                    0                         1   \n",
      "2          2387.5588                    0                         1   \n",
      "3           161.1236                    0                         1   \n",
      "4          1981.4208                    0                         0   \n",
      "\n",
      "   Item_Fat_Content_Regular  Item_Fat_Content_low fat  Item_Fat_Content_reg  \\\n",
      "0                         0                         0                     0   \n",
      "1                         0                         0                     0   \n",
      "2                         0                         0                     0   \n",
      "3                         0                         0                     0   \n",
      "4                         1                         0                     0   \n",
      "\n",
      "   ...  Outlet_Size_High  Outlet_Size_Medium  Outlet_Size_Small  \\\n",
      "0  ...                 0                   0                  1   \n",
      "1  ...                 0                   0                  1   \n",
      "2  ...                 0                   1                  0   \n",
      "3  ...                 0                   0                  0   \n",
      "4  ...                 0                   0                  1   \n",
      "\n",
      "   Outlet_Location_Type_Tier 1  Outlet_Location_Type_Tier 2  \\\n",
      "0                            0                            1   \n",
      "1                            1                            0   \n",
      "2                            0                            0   \n",
      "3                            0                            0   \n",
      "4                            0                            1   \n",
      "\n",
      "   Outlet_Location_Type_Tier 3  Outlet_Type_Grocery Store  \\\n",
      "0                            0                          0   \n",
      "1                            0                          0   \n",
      "2                            1                          0   \n",
      "3                            1                          1   \n",
      "4                            0                          0   \n",
      "\n",
      "   Outlet_Type_Supermarket Type1  Outlet_Type_Supermarket Type2  \\\n",
      "0                              1                              0   \n",
      "1                              1                              0   \n",
      "2                              0                              0   \n",
      "3                              0                              0   \n",
      "4                              1                              0   \n",
      "\n",
      "   Outlet_Type_Supermarket Type3  \n",
      "0                              0  \n",
      "1                              0  \n",
      "2                              1  \n",
      "3                              0  \n",
      "4                              0  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "\n",
      "Shape of training data : (1364, 36)\n",
      "\n",
      "Shape of testing data : (341, 36)\n",
      "\n",
      "Coefficient of model : [-6.04751631e-01  3.71764055e+02  1.62647248e+01  5.82663370e+01\n",
      " -6.00585341e+01  1.38608884e+02  2.24089684e+02 -2.35895196e+02\n",
      " -6.67448379e+01  1.11148149e+02 -3.79761821e+02  2.27052603e+02\n",
      " -5.14648815e+01  5.18168385e+00  4.64395848e+01 -1.20313643e+02\n",
      "  1.51715480e+02 -6.19901735e+01  1.11333332e+02 -1.03971682e+02\n",
      "  3.69886943e+02 -6.52646923e+01  3.25150629e+00 -6.07183361e+01\n",
      " -1.82524051e+02  1.18463867e+03  1.77389920e+02 -1.50978301e+01\n",
      "  3.51509654e+02 -7.08894978e+01 -2.80620156e+02 -1.41215267e+03\n",
      " -1.34632460e+02 -1.02123430e+03  2.56801943e+03]\n",
      "\n",
      "Intercept of model -116691.71714635314\n",
      "\n",
      "Item_Outlet_Sales on training data [1533.13780717 2418.8897328  -421.95489876 ... 1458.51700735 2659.45187975\n",
      " 2495.13927103]\n",
      "\n",
      "RMSE on train dataset :  1124.0770960226203\n",
      "\n",
      "Item_Outlet_Sales on test data [ 1.62334237e+03  8.17310308e+02  2.66717337e+03  1.38621105e+03\n",
      "  3.37436234e+03  7.05462298e+02  2.75720659e+03  4.45818500e+03\n",
      "  2.75952400e+03  1.02155767e+03  1.56259236e+03  4.84889739e+03\n",
      "  2.06999934e+03  8.90148434e+02  1.68444763e+03  2.68659173e+03\n",
      "  7.43690674e+02  2.77433946e+03  1.99467749e+03  2.74264703e+03\n",
      "  1.64451888e+03  3.68668894e+03  7.85305553e+02  1.90430835e+03\n",
      "  1.26067113e+03 -4.12203927e+01 -7.80522351e+01  9.67562892e+02\n",
      "  1.52333125e+03  2.66282245e+03  1.57437387e+03  1.02319573e+03\n",
      "  4.14522519e+03 -1.08012312e+03  1.77749817e+03  3.58646803e+03\n",
      "  3.38554347e+03  9.38320720e+02  3.46976998e+03  3.19166823e+03\n",
      "  1.76556741e+03  9.93280170e+02  5.71945495e+02  1.78747539e+03\n",
      " -1.36125650e+03 -2.98724139e+02  9.76028805e+02  2.15016340e+03\n",
      "  2.19382967e+03  3.12855666e+03  1.94853999e+03  3.88478377e+03\n",
      "  3.16609740e+03  4.02657813e+03  2.37948266e+03  7.57657914e+02\n",
      "  2.65090727e+03  8.51419071e+01  1.76795945e+03 -1.36996480e+02\n",
      "  4.53996378e+03  3.13550100e+03  2.87533756e+03  2.77162991e+03\n",
      "  2.98489498e+03  3.26432491e+03  1.47451732e+03  1.83309311e+03\n",
      "  4.20639760e+03  2.58484900e+03  2.88488640e+03  5.14541645e+02\n",
      "  2.43469032e+03 -8.35561261e+02  2.80015421e+03  1.23842953e+03\n",
      "  1.47680386e+03  4.43742544e+03  2.37366926e+03  2.53036899e+03\n",
      "  3.26987805e+03  4.56319255e+03  7.17650830e+02  1.32667263e+03\n",
      "  2.23715507e+03  8.52637717e+02  1.23590633e+03  5.73852143e+03\n",
      "  3.64812975e+03  2.16318229e+03  8.28125049e+02  3.78177475e+03\n",
      " -5.97577624e+02  1.15077996e+03  1.47597115e+03  3.18196655e+03\n",
      "  2.07709658e+03  2.18176604e+03  4.17052900e+03  4.03530667e+03\n",
      "  3.24105009e+03  7.94415182e+02  3.54160321e+03  2.64450471e+03\n",
      "  6.90613437e+02  8.46984635e+02  3.56557294e+03  2.37749337e+03\n",
      "  3.50683579e+03  1.58350511e+03  3.54589658e+03  1.86015823e+03\n",
      " -1.21787396e+03  1.90541630e+03  2.78092293e+03  2.77882229e+03\n",
      "  2.03944135e+03  2.92317822e+03  2.11217901e+03  2.87615784e+03\n",
      "  2.46277813e+03  1.84294445e+03  3.66262518e+03  7.78868356e+02\n",
      "  2.74561471e+03  3.97674933e+03  1.89932053e+03  4.38302253e+02\n",
      "  6.57271279e+02  1.49304676e+03  2.85828327e+03  3.31834325e+03\n",
      "  3.98176418e+03  4.28750285e+03  4.14789291e+03  2.44087465e+03\n",
      "  2.53940995e+03  4.49979313e+00  4.01403947e+03  1.42507497e+03\n",
      "  4.29493197e+02 -4.16996626e+02  3.23680069e+03  3.42792186e+03\n",
      "  2.65608048e+03  3.26588960e+03  1.20736603e+03  1.18969063e+03\n",
      "  3.48572050e+03  1.98007113e+03  2.71269225e+03  2.21645407e+03\n",
      "  5.53404503e+03  3.18959325e+03  4.38891547e+03  5.28826830e+02\n",
      "  1.34315736e+03  8.11980355e+02  2.41540929e+03  2.24797219e+03\n",
      "  4.90792562e+03  1.52270618e+03  2.88464187e+03  2.63461017e+03\n",
      "  6.45860450e+02  3.08172591e+03  4.10511724e+03  2.86812777e+03\n",
      "  2.78539786e+03  2.91035859e+03  3.50946833e+03  2.25592325e+03\n",
      "  5.61950185e+02  1.46079490e+03  1.05343739e+03  3.29935943e+03\n",
      "  7.28588545e+02  2.16157573e+03  2.52511648e+03  1.34587969e+03\n",
      "  2.19942657e+03  1.44565306e+03  4.67405477e+03  3.42206818e+03\n",
      "  1.61428033e+03  4.35426544e+03  3.30759326e+03  4.10023400e+03\n",
      "  1.22551973e+03  2.78132281e+03  3.31870775e+03  3.78586230e+03\n",
      " -2.61694662e+02  3.48783170e+03  2.85617098e+03  1.45921631e+03\n",
      "  1.81563801e+03  2.40721652e+03  1.33984962e+03  3.18554921e+03\n",
      "  1.38462994e+03  3.55613715e+03  1.34024523e+03  1.83418455e+03\n",
      "  3.15779437e+03  1.53208555e+03  2.06932255e+03  4.33024506e+03\n",
      "  2.01505993e+03  2.00787121e+03  1.63480708e+03  3.30242320e+03\n",
      "  8.82896797e+02  5.47271266e+03  2.53034480e+03  2.22434102e+03\n",
      "  1.25326251e+03  1.39948960e+03  6.99113608e+02  1.91025788e+03\n",
      "  3.63692227e+03  1.41227264e+03  2.89237070e+03  1.92369625e+03\n",
      "  2.69729358e+03  7.84801615e+02  7.23373321e+01  1.63449261e+03\n",
      "  7.58960123e+02  3.54319941e+03  3.81184170e+03  2.92306833e+03\n",
      "  1.62298386e+03 -2.44470387e+02  2.12635969e+03  2.93122336e+03\n",
      "  1.68798543e+03  6.46139793e+02  1.80113577e+03  1.83779819e+03\n",
      "  2.97153891e+03  2.78857906e+03  1.99514172e+03  1.40711927e+03\n",
      "  1.62422573e+03  3.00095135e+03  2.61074835e+03  9.01298096e+02\n",
      "  1.42705409e+03  2.04041892e+03  3.61368882e+03 -5.52090291e+02\n",
      "  3.62913029e+03  2.32874904e+03 -6.10835474e+02  5.08114596e+03\n",
      "  2.80788917e+03  4.59300835e+03  2.72301891e+02  1.41309667e+03\n",
      "  3.81390417e+03  4.95947446e+03  2.77481524e+03  2.59523951e+03\n",
      "  2.77074862e+03  2.11175844e+02  3.21263371e+03  1.34769672e+03\n",
      "  4.27591564e+03  1.17488624e+03  3.37264216e+03  4.10960543e+03\n",
      "  2.37125178e+03]\n",
      "\n",
      "RMSE on test dataset :  1198.2835807617575\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv(\"Dataset/train.csv\")\n",
    "test_data = pd.read_csv(\"Dataset/test.csv\")\n",
    "\n",
    "print(train_data.head())\n",
    "\n",
    "# shape of the dataset\n",
    "print('\\nShape of training data :',train_data.shape)\n",
    "print('\\nShape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Item_Outlet_Sales\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "x = train_data.drop(columns=['Item_Outlet_Sales'],axis=1)\n",
    "y = train_data['Item_Outlet_Sales']\n",
    "\n",
    "# Create the object of the Linear Regression model\n",
    "# You can also add other parameters and test your code here\n",
    "# Some parameters are : fit_intercept and normalize\n",
    "# Documentation of sklearn LinearRegression: \n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2)\n",
    "model = LinearRegression()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# coefficeints of the trained model\n",
    "print('\\nCoefficient of model :', model.coef_)\n",
    "\n",
    "# intercept of the model\n",
    "print('\\nIntercept of model',model.intercept_)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('\\nItem_Outlet_Sales on training data',predict_train) \n",
    "\n",
    "# Root Mean Squared Error on training dataset\n",
    "rmse_train = mean_squared_error(train_y,predict_train)**(0.5)\n",
    "print('\\nRMSE on train dataset : ', rmse_train)\n",
    "\n",
    "# predict the target on the testing dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('\\nItem_Outlet_Sales on test data',predict_test) \n",
    "\n",
    "# Root Mean Squared Error on testing dataset\n",
    "rmse_test = mean_squared_error(test_y,predict_test)**(0.5)\n",
    "print('\\nRMSE on test dataset : ', rmse_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is used to estimate discrete values ( Binary values like 0/1, yes/no, true/false ) based on given set of independent variable(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived        Age     Fare  Pclass_1  Pclass_2  Pclass_3  Sex_female  \\\n",
      "0         0  28.500000   7.2292         0         0         1           0   \n",
      "1         1  27.000000  10.5000         0         1         0           1   \n",
      "2         1  29.699118  16.1000         0         0         1           1   \n",
      "3         0  29.699118   0.0000         1         0         0           0   \n",
      "4         0  17.000000   8.6625         0         0         1           0   \n",
      "\n",
      "   Sex_male  SibSp_0  SibSp_1  ...  Parch_0  Parch_1  Parch_2  Parch_3  \\\n",
      "0         1        1        0  ...        1        0        0        0   \n",
      "1         0        1        0  ...        1        0        0        0   \n",
      "2         0        0        1  ...        1        0        0        0   \n",
      "3         1        1        0  ...        1        0        0        0   \n",
      "4         1        1        0  ...        1        0        0        0   \n",
      "\n",
      "   Parch_4  Parch_5  Parch_6  Embarked_C  Embarked_Q  Embarked_S  \n",
      "0        0        0        0           1           0           0  \n",
      "1        0        0        0           0           0           1  \n",
      "2        0        0        0           0           0           1  \n",
      "3        0        0        0           0           0           1  \n",
      "4        0        0        0           0           0           1  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "Shape of training data : (712, 25)\n",
      "Shape of testing data : (179, 25)\n",
      "Coefficient of model : [[-0.0309277   0.0015481   0.89622701  0.06137487 -1.0059297   1.25419987\n",
      "  -1.30252768  1.05848091  0.98967102  0.7425803  -1.03750848 -0.76764104\n",
      "  -0.4245791  -0.60933143  0.3140976   0.69532878  0.09004328  0.30114498\n",
      "  -0.6955108  -0.44316222 -0.31026944  0.16940139  0.15518145 -0.37291066]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anzconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intercept of model [-0.04832782]\n",
      "Target on train data [0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1\n",
      " 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1\n",
      " 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0\n",
      " 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
      " 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 1\n",
      " 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1\n",
      " 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 1\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
      " 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 0 0 0 1\n",
      " 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 0\n",
      " 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0\n",
      " 1 0 1 1 1 0 0 0 0]\n",
      "accuracy_score on train dataset :  0.8089887640449438\n",
      "Target on test data [0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1\n",
      " 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1]\n",
      "accuracy_score on test dataset :  0.8212290502793296\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('Dataset/train-data.csv')\n",
    "test_data = pd.read_csv('Dataset/test-data.csv')\n",
    "\n",
    "\n",
    "print(train_data.head())\n",
    "\n",
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# coefficeints of the trained model\n",
    "print('Coefficient of model :', model.coef_)\n",
    "\n",
    "# intercept of the model\n",
    "print('Intercept of model',model.intercept_)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('Target on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('accuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('Target on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('accuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, it works for both categorical and continuous dependent variables. In this algorithm, we split the population into two or more homogeneous sets. This is done based on most significant attributes/ independent variables to make as distinct groups as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data : (712, 25)\n",
      "Shape of testing data : (179, 25)\n",
      "Target on train data [0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
      " 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0\n",
      " 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1\n",
      " 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0\n",
      " 0 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1\n",
      " 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1\n",
      " 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
      " 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      " 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1\n",
      " 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0\n",
      " 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0\n",
      " 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0\n",
      " 1 0 1 1 1 0 0 1 0]\n",
      "accuracy_score on train dataset :  0.9859550561797753\n",
      "Target on test data [0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0\n",
      " 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1\n",
      " 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0\n",
      " 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0]\n",
      "accuracy_score on test dataset :  0.770949720670391\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('Dataset/train-data.csv')\n",
    "test_data = pd.read_csv('Dataset/test-data.csv')\n",
    "\n",
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# depth of the decision tree\n",
    "# print('Depth of the Decision Tree :', model.get_depth())\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('Target on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('accuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('Target on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('accuracy_score on test dataset : ', accuracy_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is a classification method. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data : (712, 25)\n",
      "Shape of testing data : (179, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anzconda\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target on train data [0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1\n",
      " 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0\n",
      " 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0\n",
      " 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0\n",
      " 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
      " 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0\n",
      " 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0\n",
      " 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1\n",
      " 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0\n",
      " 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0\n",
      " 0 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1\n",
      " 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0\n",
      " 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0\n",
      " 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
      " 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0\n",
      " 1 0 1 1 1 0 0 1 0]\n",
      "accuracy_score on train dataset :  0.8497191011235955\n",
      "Target on test data [0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 1 1\n",
      " 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1\n",
      " 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0 0]\n",
      "accuracy_score on test dataset :  0.7206703910614525\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('Dataset/train-data.csv')\n",
    "test_data = pd.read_csv('Dataset/test-data.csv')\n",
    "\n",
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('Target on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('accuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('Target on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('accuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is a classification technique based on Bayes’ theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data : (712, 25)\n",
      "Shape of testing data : (179, 25)\n",
      "Target on train data [1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1]\n",
      "accuracy_score on train dataset :  0.44803370786516855\n",
      "Target on test data [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "accuracy_score on test dataset :  0.35195530726256985\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('Dataset/train-data.csv')\n",
    "test_data = pd.read_csv('Dataset/test-data.csv')\n",
    "\n",
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('Target on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('accuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('Target on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('accuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN (k- Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It can be used for both classification and regression problems. However, it is more widely used in classification problemsin the industry. K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. The case being assigned to the class is most common amongst its K nearest neighbors measured by a distance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('train-data.csv')\n",
    "test_data = pd.read_csv('test-data.csv')\n",
    "\n",
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']\n",
    "\n",
    "model = KNeighborsClassifier()  \n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# Number of Neighbors used to predict the target\n",
    "print('\\nThe number of neighbors used to predict the target : ',model.n_neighbors)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('\\nTarget on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('accuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('Target on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('accuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is a type of unsupervised algorithm which solves the clustering problem. Its procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters). Data points inside a cluster are homogeneous and heterogeneous to peer groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data : (712, 25)\n",
      "Shape of testing data : (179, 25)\n",
      "\n",
      "Default number of Clusters :  8\n",
      "\n",
      "CLusters on train data [7 7 7 7 7 7 0 7 7 0 5 7 3 3 5 0 0 3 0 7 7 0 0 0 7 7 7 4 7 7 4 5 7 7 0 7 7\n",
      " 7 7 7 0 7 7 4 7 5 7 7 4 5 6 7 7 7 3 4 7 7 3 7 7 6 6 3 5 4 7 5 3 0 7 7 4 7\n",
      " 7 1 3 0 7 7 7 5 4 0 1 4 0 7 7 3 4 5 3 3 0 4 5 7 3 0 7 4 7 3 5 3 7 7 5 4 5\n",
      " 7 3 5 7 7 0 4 7 7 3 3 7 3 7 7 4 2 7 0 7 1 0 7 4 3 0 6 7 5 7 7 0 0 7 6 1 7\n",
      " 7 7 0 4 5 7 1 5 0 5 4 7 7 7 7 4 4 7 7 3 4 7 7 0 7 5 4 7 5 7 7 5 7 7 7 7 7\n",
      " 7 4 0 1 5 7 7 6 7 4 7 7 7 4 4 3 7 7 0 4 3 4 5 7 7 7 5 5 7 7 7 7 7 7 7 5 7\n",
      " 7 7 4 5 7 7 6 7 4 5 4 7 7 7 3 7 7 7 3 0 0 7 7 3 6 4 7 7 6 0 7 3 3 5 7 0 3\n",
      " 4 5 7 4 5 5 1 4 4 5 5 5 4 5 7 0 4 4 7 7 7 7 7 0 3 7 4 3 1 0 1 3 0 7 4 3 7\n",
      " 7 7 0 0 3 7 7 0 5 0 3 7 0 7 7 7 3 7 1 1 7 7 5 7 4 5 1 5 0 7 7 5 7 7 7 5 6\n",
      " 7 0 5 5 0 7 4 5 0 7 5 0 7 7 7 7 7 7 3 0 7 3 5 7 3 7 7 7 7 7 7 5 7 5 7 0 7\n",
      " 7 7 7 1 7 5 5 5 7 4 5 5 7 7 3 7 7 1 7 7 4 3 5 4 7 0 0 3 7 7 7 1 7 1 4 3 7\n",
      " 3 2 7 7 7 0 7 7 7 7 5 7 7 0 7 3 7 0 1 0 0 4 0 7 5 7 7 1 7 4 7 7 7 7 4 7 3\n",
      " 7 7 7 7 7 3 7 7 3 7 0 5 5 0 5 0 4 3 7 7 5 7 5 7 5 7 7 7 7 1 3 7 7 5 0 5 4\n",
      " 0 7 3 5 3 5 7 7 5 4 1 4 5 7 4 7 7 0 7 3 7 7 7 5 7 7 7 3 5 7 3 7 7 7 2 4 5\n",
      " 7 7 7 4 7 7 0 7 7 7 5 4 7 7 4 1 7 7 7 4 5 3 7 4 5 7 5 0 3 3 7 4 3 5 7 7 4\n",
      " 7 1 5 7 5 7 4 7 7 7 3 7 5 7 7 6 4 7 4 5 7 7 5 7 1 1 7 5 7 4 1 5 7 4 5 4 7\n",
      " 7 7 0 3 0 3 5 3 7 7 7 6 7 7 7 7 0 4 4 4 7 4 3 7 7 7 4 0 3 0 7 7 4 7 5 5 4\n",
      " 3 7 7 4 7 4 7 5 7 7 5 0 7 7 5 7 4 4 3 0 7 7 0 1 7 4 0 3 7 7 0 7 5 3 7 7 7\n",
      " 5 7 7 7 5 7 7 7 7 5 3 5 4 7 7 3 6 7 0 7 5 7 7 4 7 7 4 7 7 7 7 0 5 4 7 7 7\n",
      " 4 7 7 7 4 7 6 7 7]\n",
      "Clusters on test data [7 7 7 1 4 7 7 0 7 7 7 3 5 5 7 7 5 3 7 7 7 7 5 7 3 7 7 4 0 4 4 7 3 7 7 7 7\n",
      " 6 7 7 0 5 7 7 4 5 7 7 4 4 4 5 7 4 3 1 3 7 6 5 7 4 7 7 0 7 5 7 7 5 3 0 7 0\n",
      " 7 7 7 3 5 7 7 7 4 1 7 7 1 0 7 7 7 7 5 7 4 7 7 5 4 7 3 4 7 0 7 4 7 1 5 5 5\n",
      " 0 4 7 7 4 0 5 7 4 1 3 4 7 7 7 4 5 7 3 5 7 7 1 5 7 7 5 6 0 7 7 5 0 7 7 7 0\n",
      " 7 0 7 0 4 0 7 7 7 5 7 7 7 7 5 7 7 5 4 7 7 7 7 3 0 7 0 7 0 0 5]\n",
      "\n",
      "Number of Clusters :  3\n",
      "\n",
      "CLusters on train data [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 2 0 0 0 0 1 0 0 0 0 0 2 2 0 0 1 0 0 0 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 2 0 0 0 1 0 0 1 0 0 2 0 0 0 0 0 0 0 2 1 0\n",
      " 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 2 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 2 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 2 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 2\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0\n",
      " 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 2 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
      " 1 0 0 0 1 0 2 0 0]\n",
      "Clusters on test data [0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0\n",
      " 2 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 1 0 0 2 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0\n",
      " 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('Dataset/train-data.csv')\n",
    "test_data = pd.read_csv('Dataset/test-data.csv')\n",
    "\n",
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to divide the training data into differernt clusters\n",
    "# and predict in which cluster a particular data point belongs.  \n",
    "\n",
    "'''\n",
    "Some parameters are : n_clusters and max_iter\n",
    " '''\n",
    "\n",
    "model = KMeans()  \n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_data)\n",
    "\n",
    "# Number of Clusters\n",
    "print('\\nDefault number of Clusters : ',model.n_clusters)\n",
    "\n",
    "# predict the clusters on the train dataset\n",
    "predict_train = model.predict(train_data)\n",
    "print('\\nCLusters on train data',predict_train) \n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_data)\n",
    "print('Clusters on test data',predict_test) \n",
    "\n",
    "# Now, we will train a model with n_cluster = 3\n",
    "model_n3 = KMeans(n_clusters=3)\n",
    "\n",
    "# fit the model with the training data\n",
    "model_n3.fit(train_data)\n",
    "\n",
    "# Number of Clusters\n",
    "print('\\nNumber of Clusters : ',model_n3.n_clusters)\n",
    "\n",
    "# predict the clusters on the train dataset\n",
    "predict_train_3 = model_n3.predict(train_data)\n",
    "print('\\nCLusters on train data',predict_train_3) \n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test_3 = model_n3.predict(test_data)\n",
    "print('Clusters on test data',predict_test_3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest is a trademark term for an ensemble of decision trees. In Random Forest, we’ve collection of decision trees (so known as “Forest”). To classify a new object based on attributes, each tree gives a classification and we say the tree “votes” for that class. The forest chooses the classification having the most votes (over all the trees in the forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived        Age     Fare  Pclass_1  Pclass_2  Pclass_3  Sex_female  \\\n",
      "0         0  28.500000   7.2292         0         0         1           0   \n",
      "1         1  27.000000  10.5000         0         1         0           1   \n",
      "2         1  29.699118  16.1000         0         0         1           1   \n",
      "\n",
      "   Sex_male  SibSp_0  SibSp_1  ...  Parch_0  Parch_1  Parch_2  Parch_3  \\\n",
      "0         1        1        0  ...        1        0        0        0   \n",
      "1         0        1        0  ...        1        0        0        0   \n",
      "2         0        0        1  ...        1        0        0        0   \n",
      "\n",
      "   Parch_4  Parch_5  Parch_6  Embarked_C  Embarked_Q  Embarked_S  \n",
      "0        0        0        0           1           0           0  \n",
      "1        0        0        0           0           0           1  \n",
      "2        0        0        0           0           0           1  \n",
      "\n",
      "[3 rows x 25 columns]\n",
      "\n",
      "Shape of training data : (712, 25)\n",
      "\n",
      "Shape of testing data : (179, 25)\n",
      "Number of Trees used :  10\n",
      "\n",
      "Target on train data [0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
      " 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0\n",
      " 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0\n",
      " 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1\n",
      " 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0\n",
      " 0 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1\n",
      " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0\n",
      " 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
      " 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      " 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1\n",
      " 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0\n",
      " 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 0\n",
      " 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0\n",
      " 1 0 1 1 1 0 0 1 0]\n",
      "\n",
      "accuracy_score on train dataset :  0.973314606741573\n",
      "\n",
      "Target on test data [0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0]\n",
      "\n",
      "accuracy_score on test dataset :  0.8100558659217877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anzconda\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('train-data.csv')\n",
    "test_data = pd.read_csv('test-data.csv')\n",
    "\n",
    "# view the top 3 rows of the dataset\n",
    "print(train_data.head(3))\n",
    "\n",
    "# shape of the dataset\n",
    "print('\\nShape of training data :',train_data.shape)\n",
    "print('\\nShape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']\n",
    "\n",
    "'''\n",
    "Some parameters are : n_estimators and max_depth\n",
    "'''\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# number of trees used\n",
    "print('Number of Trees used : ', model.n_estimators)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('\\nTarget on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('\\naccuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('\\nTarget on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('\\naccuracy_score on test dataset : ', accuracy_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How’d you identify highly significant variable(s) out 1000 or 2000? In such cases, dimensionality reduction algorithm helps us along with various other algorithms like Decision Tree, Random Forest, PCA, Factor Analysis, Identify based on correlation matrix, missing value ratio and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Item_Weight  Item_Visibility  Item_MRP  Outlet_Establishment_Year  \\\n",
      "0     6.800000         0.037490   48.6034                       2004   \n",
      "1    15.600000         0.172597  114.8518                       1997   \n",
      "2    12.911575         0.054721  107.8254                       1985   \n",
      "\n",
      "   Item_Outlet_Sales  Item_Fat_Content_LF  Item_Fat_Content_Low Fat  \\\n",
      "0           291.6204                    0                         1   \n",
      "1          2163.1842                    0                         1   \n",
      "2          2387.5588                    0                         1   \n",
      "\n",
      "   Item_Fat_Content_Regular  Item_Fat_Content_low fat  Item_Fat_Content_reg  \\\n",
      "0                         0                         0                     0   \n",
      "1                         0                         0                     0   \n",
      "2                         0                         0                     0   \n",
      "\n",
      "   ...  Outlet_Size_High  Outlet_Size_Medium  Outlet_Size_Small  \\\n",
      "0  ...                 0                   0                  1   \n",
      "1  ...                 0                   0                  1   \n",
      "2  ...                 0                   1                  0   \n",
      "\n",
      "   Outlet_Location_Type_Tier 1  Outlet_Location_Type_Tier 2  \\\n",
      "0                            0                            1   \n",
      "1                            1                            0   \n",
      "2                            0                            0   \n",
      "\n",
      "   Outlet_Location_Type_Tier 3  Outlet_Type_Grocery Store  \\\n",
      "0                            0                          0   \n",
      "1                            0                          0   \n",
      "2                            1                          0   \n",
      "\n",
      "   Outlet_Type_Supermarket Type1  Outlet_Type_Supermarket Type2  \\\n",
      "0                              1                              0   \n",
      "1                              1                              0   \n",
      "2                              0                              0   \n",
      "\n",
      "   Outlet_Type_Supermarket Type3  \n",
      "0                              0  \n",
      "1                              0  \n",
      "2                              1  \n",
      "\n",
      "[3 rows x 36 columns]\n",
      "\n",
      "Shape of training data : (1364, 36)\n",
      "\n",
      "Shape of testing data : (341, 36)\n",
      "\n",
      "Training model with 35 dimensions.\n",
      "\n",
      "RMSE on train dataset :  1135.8159344155245\n",
      "\n",
      "RMSE on test dataset :  1009.2517232209694\n",
      "\n",
      "Training model with 12 dimensions.\n",
      "\n",
      "RMSE on new train dataset :  1159.9275763999258\n",
      "\n",
      "RMSE on new test dataset :  1014.4082831412395\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error  \n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('Dataset/train.csv')\n",
    "test_data = pd.read_csv('Dataset/test.csv')\n",
    "\n",
    "# view the top 3 rows of the dataset\n",
    "print(train_data.head(3))\n",
    "\n",
    "# shape of the dataset\n",
    "print('\\nShape of training data :',train_data.shape)\n",
    "print('\\nShape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "# target variable - Item_Outlet_Sales\n",
    "train_x = train_data.drop(columns=['Item_Outlet_Sales'],axis=1)\n",
    "train_y = train_data['Item_Outlet_Sales']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Item_Outlet_Sales'],axis=1)\n",
    "test_y = test_data['Item_Outlet_Sales']\n",
    "\n",
    "print('\\nTraining model with {} dimensions.'.format(train_x.shape[1]))\n",
    "\n",
    "# create object of model\n",
    "model = LinearRegression()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "\n",
    "# Accuray Score on train dataset\n",
    "rmse_train = mean_squared_error(train_y,predict_train)**(0.5)\n",
    "print('\\nRMSE on train dataset : ', rmse_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "rmse_test = mean_squared_error(test_y,predict_test)**(0.5)\n",
    "print('\\nRMSE on test dataset : ', rmse_test)\n",
    "\n",
    "# create the object of the PCA (Principal Component Analysis) model\n",
    "# reduce the dimensions of the data to 12\n",
    "'''\n",
    "Some parameters are : svd_solver, iterated_power\n",
    "'''\n",
    "model_pca = PCA(n_components=12)\n",
    "\n",
    "new_train = model_pca.fit_transform(train_x)\n",
    "new_test  = model_pca.fit_transform(test_x)\n",
    "\n",
    "print('\\nTraining model with {} dimensions.'.format(new_train.shape[1]))\n",
    "\n",
    "# create object of model\n",
    "model_new = LinearRegression()\n",
    "\n",
    "# fit the model with the training data\n",
    "model_new.fit(new_train,train_y)\n",
    "\n",
    "# predict the target on the new train dataset\n",
    "predict_train_pca = model_new.predict(new_train)\n",
    "\n",
    "# Accuray Score on train dataset\n",
    "rmse_train_pca = mean_squared_error(train_y,predict_train_pca)**(0.5)\n",
    "print('\\nRMSE on new train dataset : ', rmse_train_pca)\n",
    "\n",
    "# predict the target on the new test dataset\n",
    "predict_test_pca = model_new.predict(new_test)\n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "rmse_test_pca = mean_squared_error(test_y,predict_test_pca)**(0.5)\n",
    "print('\\nRMSE on new test dataset : ', rmse_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM is a boosting algorithm used when we deal with plenty of data to make a prediction with high prediction power. Boosting is actually an ensemble of learning algorithms which combines the prediction of several base estimators in order to improve robustness over a single estimator. It combines multiple weak or average predictors to a build strong predictor. These boosting algorithms always work well in data science competitions like Kaggle, AV Hackathon, CrowdAnalytix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data : (712, 25)\n",
      "Shape of testing data : (179, 25)\n",
      "\n",
      "Target on train data [0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
      " 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0\n",
      " 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1\n",
      " 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0\n",
      " 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1\n",
      " 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
      " 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      " 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1\n",
      " 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0\n",
      " 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0\n",
      " 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0\n",
      " 1 0 1 1 1 0 0 1 0]\n",
      "\n",
      "accuracy_score on train dataset :  0.9620786516853933\n",
      "\n",
      "Target on test data [0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 0 1\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1\n",
      " 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
      " 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0]\n",
      "\n",
      "accuracy_score on test dataset :  0.8268156424581006\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('train-data.csv')\n",
    "test_data = pd.read_csv('test-data.csv')\n",
    "\n",
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']\n",
    "\n",
    "'''\n",
    "Some parameters are : learning_rate, n_estimators\n",
    "'''\n",
    "model = GradientBoostingClassifier(n_estimators=100,max_depth=5)\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('\\nTarget on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('\\naccuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('\\nTarget on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('\\naccuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The XGBoost has an immensely high predictive power which makes it the best choice for accuracy in events as it possesses both linear model and the tree learning algorithm, making the algorithm almost 10x faster than existing gradient booster techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in e:\\anzconda\\lib\\site-packages (0.90)\n",
      "Requirement already satisfied: numpy in e:\\anzconda\\lib\\site-packages (from xgboost) (1.16.2)\n",
      "Requirement already satisfied: scipy in e:\\anzconda\\lib\\site-packages (from xgboost) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data : (712, 25)\n",
      "Shape of testing data : (179, 25)\n",
      "\n",
      "Target on train data [0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 1 0 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1\n",
      " 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0\n",
      " 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
      " 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0\n",
      " 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1\n",
      " 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1\n",
      " 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0\n",
      " 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 1 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0\n",
      " 1 0 1 1 1 0 0 1 0]\n",
      "\n",
      "accuracy_score on train dataset :  0.8693820224719101\n",
      "\n",
      "Target on test data [0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0]\n",
      "\n",
      "accuracy_score on test dataset :  0.8212290502793296\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('Dataset/train-data.csv')\n",
    "test_data = pd.read_csv('Dataset/test-data.csv')\n",
    "\n",
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']\n",
    "\n",
    "'''\n",
    "Some parameters are : max_depth and n_estimators\n",
    "'''\n",
    "model = XGBClassifier()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('\\nTarget on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('\\naccuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('\\nTarget on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('\\naccuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster training speed and higher efficiency\n",
    "Lower memory usage\n",
    "Better accuracy\n",
    "Parallel and GPU learning supported\n",
    "Capable of handling large-scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(500, 10) # 500 entities, each contains 10 features\n",
    "label = np.random.randint(2, size=500) # binary target\n",
    "\n",
    "train_data = lgb.Dataset(data, label=label)\n",
    "test_data = train_data.create_valid('test.svm')\n",
    "\n",
    "param = {'num_leaves':31, 'num_trees':100, 'objective':'binary'}\n",
    "param['metric'] = 'auc'\n",
    "\n",
    "num_round = 10\n",
    "bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "bst.save_model('model.txt')\n",
    "\n",
    "# 7 entities, each contains 10 features\n",
    "data = np.random.rand(7, 10)\n",
    "ypred = bst.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best part about CatBoost is that it does not require extensive data training like other ML models, and can work on a variety of data formats; not undermining how robust it can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-af77dd8ea354>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#Read training and testing files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "#Read training and testing files\n",
    "train = pd.read_csv(\"Dataset/train.csv\")\n",
    "test = pd.read_csv(\"Dataset/test.csv\")\n",
    "\n",
    "#Imputing missing values for both train and test\n",
    "train.fillna(-999, inplace=True)\n",
    "test.fillna(-999,inplace=True)\n",
    "\n",
    "#Creating a training set for modeling and validation set to check model performance\n",
    "X = train.drop(['Item_Outlet_Sales'], axis=1)\n",
    "y = train.Item_Outlet_Sales\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.7, random_state=1234)\n",
    "categorical_features_indices = np.where(X.dtypes != np.float)[0]\n",
    "\n",
    "#importing library and building model\n",
    "from catboost import CatBoostRegressor\n",
    "model = CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function='RMSE')\n",
    "\n",
    "model.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_validation, y_validation),plot=True)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['Item_Identifier'] = test['Item_Identifier']\n",
    "submission['Outlet_Identifier'] = test['Outlet_Identifier']\n",
    "submission['Item_Outlet_Sales'] = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
